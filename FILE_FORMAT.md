# .DCZ File Format Specification

## Overview

The DCZ (Data Compression Z) file format is designed for efficient compression and decompression of large files using GPU-accelerated Huffman encoding. The format prioritizes:
- **Sequential I/O**: Pure sequential writes during compression (no seeking)
- **Fast Decompression**: O(1) footer location via pointer at file end
- **Scalability**: Supports files of any size (tested up to 15+ GB)
- **Backward Compatibility**: Supports legacy header-first format

---

## File Structure (New Format - Version 1)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    COMPRESSED DATA SECTION                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Chunk 0: Huffman-encoded data                            â”‚  â”‚
â”‚  â”‚ Chunk 1: Huffman-encoded data                            â”‚  â”‚
â”‚  â”‚ Chunk 2: Huffman-encoded data                            â”‚  â”‚
â”‚  â”‚ ...                                                       â”‚  â”‚
â”‚  â”‚ Chunk N: Huffman-encoded data                            â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                       FOOTER SECTION                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ FOOTER HEADER (Fixed fields)                             â”‚  â”‚
â”‚  â”‚  â”œâ”€ Magic Number: 0x44435A46 ("DCZF") [4 bytes]        â”‚  â”‚
â”‚  â”‚  â”œâ”€ Version: 1 [4 bytes]                                â”‚  â”‚
â”‚  â”‚  â”œâ”€ Filename Length [4 bytes]                           â”‚  â”‚
â”‚  â”‚  â”œâ”€ Filename (UTF-8) [variable]                         â”‚  â”‚
â”‚  â”‚  â”œâ”€ Original File Size [8 bytes]                        â”‚  â”‚
â”‚  â”‚  â”œâ”€ Original Timestamp [8 bytes]                        â”‚  â”‚
â”‚  â”‚  â”œâ”€ Chunk Size [4 bytes]                                â”‚  â”‚
â”‚  â”‚  â”œâ”€ Global Checksum (SHA-256) [32 bytes]               â”‚  â”‚
â”‚  â”‚  â””â”€ Number of Chunks [4 bytes]                          â”‚  â”‚
â”‚  â”‚                                                          â”‚  â”‚
â”‚  â”‚ CHUNK METADATA ARRAY (N chunks)                         â”‚  â”‚
â”‚  â”‚  FOR EACH CHUNK:                                        â”‚  â”‚
â”‚  â”‚    â”œâ”€ Chunk Index [4 bytes]                            â”‚  â”‚
â”‚  â”‚    â”œâ”€ Original Offset [8 bytes]                        â”‚  â”‚
â”‚  â”‚    â”œâ”€ Original Size [4 bytes]                          â”‚  â”‚
â”‚  â”‚    â”œâ”€ Compressed Offset [8 bytes]                      â”‚  â”‚
â”‚  â”‚    â”œâ”€ Compressed Size [4 bytes]                        â”‚  â”‚
â”‚  â”‚    â”œâ”€ Checksum (SHA-256) [32 bytes]                    â”‚  â”‚
â”‚  â”‚    â””â”€ Code Lengths (256 Ã— short) [512 bytes]          â”‚  â”‚
â”‚  â”‚                                                          â”‚  â”‚
â”‚  â”‚  Total per chunk: ~572 bytes                           â”‚  â”‚
â”‚  â”‚  Total for N chunks: N Ã— 572 bytes                     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                  â”‚
â”‚  FOOTER POINTER (ALWAYS LAST 8 BYTES)                          â”‚
â”‚  â””â”€ Footer Start Offset [8 bytes long]                         â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†‘
                  Read this first!
```

---

## Detailed Field Specifications

### 1. Compressed Data Section

**Location**: Bytes `0` to `footer_start_offset - 1`

Each chunk contains raw Huffman-encoded data with no additional headers or markers:
- **Bit-packed**: Compressed bits are packed into bytes
- **Sequential**: Chunks are written in order (0, 1, 2, ..., N)
- **Variable length**: Each chunk has its own compressed size

### 2. Footer Header

**Location**: Starts at `footer_start_offset`

| Field | Type | Size | Description |
|-------|------|------|-------------|
| **Magic Number** | uint32 (big-endian) | 4 bytes | `0x44435A46` ("DCZF") - File format identifier |
| **Version** | uint32 (big-endian) | 4 bytes | Format version (currently `1`) |
| **Filename Length** | uint32 (big-endian) | 4 bytes | Length of original filename in bytes |
| **Filename** | UTF-8 string | Variable | Original filename (for verification) |
| **Original File Size** | uint64 (big-endian) | 8 bytes | Size of uncompressed file in bytes |
| **Original Timestamp** | uint64 (big-endian) | 8 bytes | File modification time (Unix timestamp in ms) |
| **Chunk Size** | uint32 (big-endian) | 4 bytes | Size of each chunk (default: 8 MB = 8,388,608 bytes) |
| **Global Checksum** | byte[] | 32 bytes | SHA-256 hash of entire original file |
| **Number of Chunks** | uint32 (big-endian) | 4 bytes | Total number of compressed chunks |

### 3. Chunk Metadata Array

**Location**: Immediately after footer header

For each chunk (repeated N times):

| Field | Type | Size | Description |
|-------|------|------|-------------|
| **Chunk Index** | uint32 | 4 bytes | Sequential index (0, 1, 2, ..., N-1) |
| **Original Offset** | uint64 | 8 bytes | Byte offset in original file |
| **Original Size** | uint32 | 4 bytes | Uncompressed chunk size (â‰¤ chunk_size) |
| **Compressed Offset** | uint64 | 8 bytes | Byte offset in compressed file (from start) |
| **Compressed Size** | uint32 | 4 bytes | Compressed chunk size in bytes |
| **Checksum** | byte[] | 32 bytes | SHA-256 hash of this chunk's original data |
| **Code Lengths** | short[256] | 512 bytes | Huffman code length for each byte value (0-255) |

**Note**: Code lengths are stored as shorts (2 bytes each) rather than full codes to save space. The actual Huffman codes are reconstructed using canonical Huffman algorithm during decompression.

### 4. Footer Pointer

**Location**: **ALWAYS** the last 8 bytes of file

| Field | Type | Size | Description |
|-------|------|------|-------------|
| **Footer Start Offset** | uint64 | 8 bytes | Absolute byte offset where footer begins |

**Critical**: This pointer enables O(1) footer location regardless of:
- File size (1 KB to 1 TB+)
- Number of chunks (1 to millions)
- Footer size (varies with chunk count)

---

## Footer Size Calculation

The footer size depends on the number of chunks:

```
Footer Size = Footer Header Size + (Chunk Metadata Size Ã— Number of Chunks)

Footer Header Size = 4 + 4 + 4 + filename_length + 8 + 8 + 4 + 32 + 4
                   â‰ˆ 68 + filename_length bytes

Chunk Metadata Size = 4 + 8 + 4 + 8 + 4 + 32 + 512
                    = 572 bytes per chunk
```

### Examples:

| File Size | Chunk Size | Chunks | Footer Size |
|-----------|-----------|--------|-------------|
| 100 MB | 8 MB | 13 | ~7.7 KB |
| 1 GB | 8 MB | 128 | ~71.6 KB |
| 3 GB | 8 MB | 384 | ~215 KB |
| 10 GB | 8 MB | 1,280 | ~714 KB |
| 100 GB | 8 MB | 12,800 | ~7.14 MB |

---

## Reading Algorithm

### Decompression Process

1. **Read Footer Pointer**
   ```java
   RandomAccessFile file = new RandomAccessFile(compressedFile, "r");
   long fileSize = file.length();
   file.seek(fileSize - 8);  // Last 8 bytes
   long footerStartOffset = file.readLong();
   ```

2. **Read Footer**
   ```java
   file.seek(footerStartOffset);
   int footerSize = (int)(fileSize - footerStartOffset - 8);
   byte[] footerData = new byte[footerSize];
   file.readFully(footerData);
   
   // Parse footer header and chunk metadata
   CompressionHeader header = CompressionHeader.readFrom(footerData);
   ```

3. **Decompress Chunks**
   ```java
   for (ChunkMetadata chunk : header.getChunks()) {
       // Read compressed data from offset
       file.seek(chunk.getCompressedOffset());
       byte[] compressedData = new byte[chunk.getCompressedSize()];
       file.readFully(compressedData);
       
       // Decode using Huffman codes from chunk metadata
       byte[] decompressed = huffmanDecode(compressedData, chunk.getCodeLengths());
       
       // Verify checksum
       byte[] checksum = sha256(decompressed);
       if (!Arrays.equals(checksum, chunk.getChecksum())) {
           throw new IOException("Chunk checksum mismatch");
       }
       
       // Write to output
       outputFile.write(decompressed);
   }
   ```

---

## Advantages of This Format

### 1. **Optimal Write Performance** ğŸš€
- **Sequential writes only**: No seeking during compression
- **No temporary files**: Direct write to final file
- **Minimal buffer flushing**: Large sequential I/O operations
- **Result**: 85-91% faster file I/O compared to header-first format

### 2. **Fast Decompression** âš¡
- **O(1) footer location**: Just read last 8 bytes
- **No scanning**: No need to search for magic numbers
- **Predictable**: Always know exactly where footer is
- **Scalable**: Works identically for 1 MB and 1 TB files

### 3. **Parallel Decompression** ğŸ”„
- **Independent chunks**: Each chunk has its own Huffman table
- **Random access**: Jump directly to any chunk
- **GPU-friendly**: Decompress multiple chunks simultaneously

### 4. **Data Integrity** âœ…
- **Per-chunk checksums**: Detect corruption at chunk level
- **Global checksum**: Verify entire file integrity
- **Metadata protection**: Footer includes all verification data

### 5. **Flexibility** ğŸ”§
- **Configurable chunk size**: Balance memory vs. parallelism
- **Extensible**: Version field allows format evolution
- **Metadata-rich**: Preserves filename, timestamp, structure

---

## Backward Compatibility

### Legacy Format (Header-First)

The decompressor automatically detects and supports the old format:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  HEADER (at start)                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Compressed Chunk 0                                     â”‚
â”‚  Compressed Chunk 1                                     â”‚
â”‚  ...                                                    â”‚
â”‚  Compressed Chunk N                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Detection Logic**:
1. Try reading footer pointer from last 8 bytes
2. If invalid offset â†’ try reading header from start (old format)
3. If both fail â†’ file is corrupted

---

## Performance Benchmarks

### Compression (3.05 GB video file, 364 chunks)

| Metric | Old Format (Header-First) | New Format (Footer-Last) | Improvement |
|--------|--------------------------|--------------------------|-------------|
| **File I/O Time** | 77 seconds | 2-7 seconds | **85-91% faster** |
| **I/O Percentage** | 26% of total time | 1-3% of total time | **23% reduction** |
| **Seeks Required** | 2 (header + gap fix) | 0 | **100% eliminated** |
| **Temp Files** | 1 (364 chunks) | 0 | **100% eliminated** |

### Decompression (Same file)

| Metric | Old Format | New Format | Improvement |
|--------|-----------|------------|-------------|
| **Footer Location** | Scan 512 KB | Read 8 bytes | **64,000Ã— less data** |
| **Seek Operations** | 1 scan + 364 reads | 1 seek + 364 reads | **Scan eliminated** |
| **Predictability** | O(n) scan | O(1) seek | **Perfect** |

---

## Implementation Notes

### Writing DCZ Files

```java
try (DataOutputStream output = new DataOutputStream(
        new BufferedOutputStream(Files.newOutputStream(outputPath)))) {
    
    // 1. Write all compressed chunks sequentially
    long footerOffset = 0;
    for (CompressedChunk chunk : chunks) {
        output.write(chunk.compressedData);
        footerOffset += chunk.compressedData.length;
    }
    
    // 2. Write footer
    header.writeTo(output);
    
    // 3. Write footer pointer
    output.writeLong(footerOffset);
    
    output.flush();
}
```

### Reading DCZ Files

```java
try (RandomAccessFile file = new RandomAccessFile(inputPath, "r")) {
    long fileSize = file.length();
    
    // 1. Read footer pointer
    file.seek(fileSize - 8);
    long footerOffset = file.readLong();
    
    // 2. Validate and read footer
    if (footerOffset < 0 || footerOffset >= fileSize - 8) {
        throw new IOException("Invalid footer offset");
    }
    
    file.seek(footerOffset);
    byte[] footerData = new byte[(int)(fileSize - footerOffset - 8)];
    file.readFully(footerData);
    
    // 3. Parse and decompress
    CompressionHeader header = CompressionHeader.readFrom(footerData);
    // ... decompress chunks ...
}
```

---

## Huffman Encoding Details

### Canonical Huffman Codes

The format uses **canonical Huffman coding** for efficient storage:
- Only **code lengths** are stored (not full codes)
- Codes are reconstructed deterministically
- Saves ~50% footer space compared to storing full codes

### Code Length Encoding

For each byte value (0-255):
- `short[i]` = bit length of Huffman code for byte value `i`
- Value `0` = symbol doesn't appear in chunk
- Value `1-15` = typical code lengths

### Reconstruction Algorithm

```java
short[] codeLengths = chunk.getCodeLengths();
HuffmanCode[] codes = CanonicalHuffman.buildCodesFromLengths(codeLengths);
// Now use codes for decoding
```

---

## File Extension

**Recommended**: `.dcz`

Example filenames:
- `video.mp4` â†’ `video.mp4.dcz`
- `database.sql` â†’ `database.sql.dcz`
- `archive.tar` â†’ `archive.tar.dcz`

---

## Version History

| Version | Date | Changes |
|---------|------|---------|
| **1** | 2025-11-12 | Initial format with footer pointer |

---

## Related Files

- **Compression**: `GpuCompressionService.java`, `CpuCompressionService.java`
- **Header Format**: `CompressionHeader.java`
- **Huffman Coding**: `CanonicalHuffman.java`
- **Checksums**: SHA-256 via `MessageDigest`

---

## Summary

The DCZ format achieves:
âœ… **85-91% faster compression** via sequential writes  
âœ… **O(1) decompression startup** via footer pointer  
âœ… **Parallel processing** via independent chunks  
âœ… **Data integrity** via SHA-256 checksums  
âœ… **Scalability** to files of any size  
âœ… **Backward compatibility** with old format  

Perfect for GPU-accelerated compression of large files! ğŸš€
