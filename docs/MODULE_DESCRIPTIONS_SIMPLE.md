# Module Descriptions (Simplified)

## 1. Core Module

The Core Module contains the fundamental algorithms and data structures that make the compression system work. At its heart is the canonical Huffman coding implementation, which creates optimal codes for compressing data. The module includes the HuffmanNode class that builds the binary tree structure needed for encoding, and the HuffmanCode class that stores the actual compression codes for each symbol. One of the most important innovations in this module is the table-based decoder, which speeds up decompression by using a lookup table instead of traversing the tree repeatedly. This optimization makes decoding 2-3 times faster than traditional methods. The module also defines how compressed files are structured, including the file header that identifies the file format using a special magic number (0xDC5A) and stores metadata like version information and original file size. Each compressed chunk includes its own metadata with compression statistics and SHA-256 checksums to verify data integrity. By keeping all core compression logic in this module, the system maintains clean separation between the algorithms and how they are executed (CPU or GPU), making the code easier to test and maintain.

## 2. Service Module

The Service Module acts as the bridge between the compression algorithms and the rest of the application. It defines clear interfaces that specify what compression and frequency analysis operations should do, without dictating how they should be implemented. This design allows the system to switch between CPU and GPU implementations seamlessly. The ServiceFactory class decides which implementation to use based on available hardware and user preferences, automatically selecting the best option. The MetricsService tracks performance by measuring how long each step takes and calculating useful statistics like throughput and compression ratios. The module is divided into two main parts: the CPU implementation uses Java's Fork/Join framework to split work across multiple processor cores, while the GPU implementation uses TornadoVM to run operations on the graphics card. This architecture makes it easy to add new compression methods or switch between different implementations without changing other parts of the code. It also simplifies testing because mock implementations can be substituted during unit tests.

## 3. Model Module (Metrics Module)

The Model Module defines the data structures that carry information between different parts of the application. The CompressionMetrics class stores all the important information about a compression or decompression operation, including how long it took, how much the file was compressed, which method was used (CPU or GPU), and whether the operation succeeded or failed. The StageMetrics class provides detailed timing information for each individual step in the compression process, such as analyzing symbol frequencies, building the Huffman tree, encoding the data, and calculating checksums. This detailed breakdown helps identify performance bottlenecks. For example, analysis showed that encoding takes 86% of the total time, making it the primary target for future optimization. These metrics objects are designed to be thread-safe and immutable, meaning they can be safely shared between different parts of the application running concurrently. The dashboard uses them to display real-time performance graphs, the benchmark module uses them to compare CPU and GPU performance, and developers use them to understand where optimizations are needed.

## 4. UI Module

The UI Module provides the graphical interface that users interact with when running the compression application. Built using JavaFX, it includes several specialized controllers that handle different aspects of the user experience. The main application window provides navigation between different functional areas through tabs. The compress controller handles the primary task of selecting files to compress or decompress, choosing whether to use CPU or GPU processing, and displaying progress with real-time speed updates. Users can cancel operations in progress and see detailed error messages if something goes wrong. The dashboard controller displays performance statistics and operation history in an easy-to-understand format with graphs and charts showing how different stages contribute to total processing time. The benchmark controller helps users compare CPU and GPU performance by running standardized tests, warming up the system first to ensure fair measurements, and then executing multiple test runs to get reliable averages. Results can be exported to CSV files for further analysis. The settings controller lets users adjust parameters like chunk size, number of parallel threads, and whether to enable GPU acceleration. Each controller focuses on one specific area of functionality, making the code easier to understand and maintain.

## 5. Utility Module

The Utility Module contains helper functions and common tools used throughout the application. The ChecksumUtil class handles data integrity verification by computing SHA-256 hashes, which act like digital fingerprints for data. This ensures that compressed data hasn't been corrupted during storage or transmission. The utility is optimized for performance and can process data in streaming mode, meaning it doesn't need to load entire files into memory at once. The TestDataGenerator class helps with testing by creating artificial datasets with specific characteristics. It can generate compressible text with repeated patterns, random binary data that can't be compressed, and various other data types that mimic real-world files. Using generated test data ensures that tests produce consistent results every time they run, making it easier to detect when code changes accidentally break something. These utility classes are designed to be simple and reusable, avoiding unnecessary complexity. They consolidate common functionality that would otherwise be duplicated across different parts of the codebase, making maintenance easier and reducing the chance of bugs.

## 6. Benchmark Module

The Benchmark Module provides systematic tools for measuring and comparing performance. The BenchmarkSuite class runs comprehensive tests that produce reliable performance measurements. It starts with a warmup phase that lets the Java virtual machine optimize the code and the GPU compile its kernels, ensuring that measurements reflect real-world performance rather than startup overhead. After warmup, it runs multiple test iterations on standardized datasets representing different types of data like text files, binary files, and random data. The module collects detailed statistics including average speed, consistency of results, and best and worst case performance. The BenchmarkResult class stores all the outcomes from each test, including timing data, compression ratios achieved, and success or failure status. For direct comparison, the BenchmarkComparison class matches up CPU and GPU results for the same data and calculates speedup factors. Results can be displayed in the console with formatted tables, exported to CSV files for use in spreadsheets, or saved as JSON for automated processing. This rigorous testing approach provides the evidence backing performance claims in the thesis, such as the finding that GPU acceleration provides 2.61 times speedup for frequency analysis but only 1.08 times overall speedup due to encoding bottlenecks. The benchmark module ensures that performance improvements are real and reproducible rather than based on single measurements that might be misleading.

## 7. Config Module

The Config Module manages all application settings and user preferences in one centralized location. The AppConfig class ensures there's only one configuration object shared throughout the entire application, preventing conflicts from multiple conflicting settings. User preferences are automatically saved to a properties file in the user's home directory, so choices persist between application sessions. The module manages various settings including chunk size (which affects memory usage and parallelism), the default compression method (CPU, GPU, or automatic selection), number of threads for parallel processing, which GPU to use in multi-GPU systems, decode table size (trading memory for speed), and logging preferences. When users change settings, the module validates them to ensure they make sense, such as checking that chunk sizes are valid and thread counts don't exceed available CPU cores. Other parts of the application can subscribe to notifications when settings change, allowing them to update immediately without restarting the application. If the configuration file is missing or corrupted, the module provides sensible defaults that ensure the application works correctly. Having all configuration in one place eliminates scattered constants throughout the code, makes it easy for users to customize performance parameters for their specific hardware, and provides a clear place to add new settings as features are developed.
