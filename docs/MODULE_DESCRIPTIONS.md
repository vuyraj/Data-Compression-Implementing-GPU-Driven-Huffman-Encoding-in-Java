# Module Descriptions

## 1. Core Module

The Core Module (`com.datacomp.core`) forms the algorithmic foundation of the compression system, encapsulating the essential data structures and algorithms required for Huffman encoding and decoding operations. This module implements canonical Huffman coding through the `CanonicalHuffman` class, which generates optimal prefix-free codes and maintains length-sorted code tables for efficient serialization. The `HuffmanNode` class provides the binary tree representation used during tree construction, supporting priority queue operations through its `Comparable` interface implementation. The `HuffmanCode` class encapsulates individual symbol-to-code mappings, storing both the bit pattern and code length for each symbol. A key innovation within this module is the `TableBasedHuffmanDecoder`, which implements O(1) lookup-based decoding by pre-computing a 12-bit decode table that maps compressed bit patterns directly to symbols, eliminating the need for repeated tree traversal and achieving 2-3× speedup over traditional decoding methods. The module also defines the file format structure through `CompressionHeader`, which stores the magic number (0xDC5A), version information, flags, and original file size, and `ChunkMetadata`, which maintains per-chunk compression statistics including compressed size, original size, Huffman tree representation, and SHA-256 checksums. This modular design ensures that core compression logic remains independent of execution environment (CPU vs GPU) and user interface concerns, promoting code reusability and facilitating unit testing of individual algorithmic components.

## 2. Service Module

The Service Module (`com.datacomp.service`) provides the architectural backbone for compression operations through well-defined interfaces and multiple implementation strategies. At its core, the `CompressionService` interface establishes a contract for compression and decompression operations, defining methods for file processing, stream handling, and metrics collection that all concrete implementations must satisfy. Similarly, the `FrequencyService` interface abstracts frequency analysis operations, enabling polymorphic selection between CPU and GPU implementations based on runtime configuration or hardware availability. The `ServiceFactory` class implements the Factory design pattern, dynamically instantiating appropriate service implementations (CPU-only, GPU-hybrid, or fully-GPU) based on system capabilities detected through TornadoVM device queries and user preferences specified in configuration settings. The `MetricsService` provides comprehensive performance monitoring, capturing detailed timing information for each processing stage (frequency analysis, tree construction, encoding, checksum calculation, I/O) and computing aggregate statistics such as throughput, speedup factors, and compression ratios. This service-oriented architecture enables clean separation of concerns, where high-level controllers and user interfaces interact with services through interfaces rather than concrete implementations, facilitating dependency injection, simplified testing through mock objects, and seamless addition of alternative compression algorithms without modifying client code. The CPU implementation submodule (`com.datacomp.service.cpu`) contains `CpuCompressionService` and `CpuFrequencyService`, which leverage Java's Fork/Join framework for parallel frequency analysis across multiple cores, while the GPU implementation submodule (`com.datacomp.service.gpu`) houses `GpuCompressionService`, `GpuFrequencyService`, and `TornadoKernels`, providing GPU-accelerated histogram computation through TornadoVM's JIT compilation of Java methods to OpenCL/CUDA kernels.

## 3. Model Module (Metrics Module)

The Model Module (`com.datacomp.model`) defines the data transfer objects and metrics structures that facilitate communication between architectural layers and provide rich performance telemetry for analysis and visualization. The `CompressionMetrics` class serves as a comprehensive container for operation statistics, capturing operation type (compression or decompression), execution timestamps, input and output sizes, compression ratio, throughput measurements, service type used (CPU or GPU), success status, and error messages when applicable. This class also defines the `OperationType` enumeration to distinguish between compression and decompression workflows. The `StageMetrics` class provides fine-grained performance profiling by recording execution time for individual pipeline stages, utilizing the `Stage` enumeration to categorize operations into FREQUENCY_ANALYSIS, TREE_CONSTRUCTION, ENCODING, DECODING, CHECKSUM, and FILE_IO phases. These metrics enable detailed bottleneck identification, as demonstrated in performance analysis where encoding was found to consume 86% of total time while frequency analysis contributed only 10.4%. The module's design follows the Data Transfer Object pattern, where metrics classes are immutable value objects with builder patterns for construction, ensuring thread-safe sharing across concurrent operations and UI updates. Metrics objects are consumed by the `MetricsService` for aggregation, the `DashboardController` for real-time visualization in the JavaFX interface, and the `BenchmarkSuite` for comparative performance evaluation between CPU and GPU implementations. This centralized metrics model eliminates redundant performance tracking code and ensures consistent measurement methodology across the entire application, supporting data-driven optimization decisions and providing users with transparent insight into compression performance characteristics.

## 4. UI Module

The UI Module (`com.datacomp.ui`) implements a comprehensive JavaFX-based graphical user interface that provides intuitive access to compression functionality, real-time performance monitoring, and interactive benchmarking capabilities. The `DataCompApp` class serves as the JavaFX application entry point, initializing the primary stage, loading FXML resources, and establishing the main window structure with navigation and content areas. The `MainViewController` orchestrates navigation between functional tabs, manages the `AppConfig` singleton for persistent settings, and implements the `ConfigurableController` interface that all child controllers must implement to receive configuration updates. The `CompressController` handles the primary compression and decompression workflows, providing file selection dialogs, service type selection (CPU vs GPU), progress indicators with real-time throughput display, operation cancellation support, and result status reporting with detailed error messages when operations fail. The `DashboardController` presents a comprehensive metrics visualization interface, displaying recent operation history in a sortable TableView, aggregate statistics (total operations, success rate, average throughput), stage-level performance breakdown through pie charts and bar graphs, and GPU utilization metrics when hardware acceleration is active. The `BenchmarkController` facilitates comparative performance evaluation by executing warmup iterations to eliminate JIT compilation overhead, running multiple measurement iterations for statistical reliability, comparing CPU vs GPU implementations on identical datasets, generating detailed benchmark reports with speedup calculations, and exporting results to CSV format for external analysis. The `SettingsController` provides configuration management for chunk size selection (8MB, 16MB, 32MB), parallel thread count adjustment, GPU enable/disable toggle, decode table size configuration, and log level selection. This modular controller design promotes maintainability through separation of concerns, where each controller manages a single functional area with clear dependencies on service and configuration layers, enabling independent testing and iterative feature development.

## 5. Utility Module

The Utility Module (`com.datacomp.util`) provides essential helper functions and cross-cutting concerns that support multiple architectural layers without introducing circular dependencies. The `ChecksumUtil` class implements cryptographic integrity verification through SHA-256 hash computation, providing methods for both byte array and streaming data checksumming to support per-chunk validation in the compressed file format. This utility achieves high performance through use of Java's built-in `MessageDigest` implementation, which leverages native code for accelerated hash computation, and supports streaming mode to avoid loading entire chunks into memory simultaneously, critical for maintaining bounded memory usage regardless of input file size. The `TestDataGenerator` class facilitates comprehensive testing by generating synthetic datasets with controlled characteristics, including compressible text data with repeated patterns and known entropy, binary data with uniform random distribution to test incompressibility detection, structured data mimicking real-world formats like JSON and XML, adversarial inputs designed to trigger worst-case behavior (all unique symbols, pathological tree structures), and scalable datasets ranging from kilobytes to gigabytes for performance and stress testing. This generator employs reproducible randomness through seeded `Random` instances, ensuring consistent test data across multiple test runs and enabling regression detection when algorithm changes inadvertently degrade compression ratio or performance. The module's utility classes are designed as stateless services with static methods where appropriate, avoiding unnecessary object allocation and simplifying usage from test cases, service implementations, and benchmarking code. By consolidating common functionality into this module, the codebase avoids duplication of checksum logic across CPU and GPU service implementations and provides a single point of maintenance for testing infrastructure, reducing technical debt and improving overall code quality.

## 6. Benchmark Module

The Benchmark Module (`com.datacomp.benchmark`) implements a rigorous performance evaluation framework that enables systematic comparison of compression implementations across varying workloads, hardware configurations, and algorithmic variants. The `BenchmarkSuite` class orchestrates comprehensive benchmark execution, beginning with a configurable warmup phase that executes multiple compression cycles to allow JIT compilation optimization and GPU kernel compilation, ensuring that subsequent measurements reflect steady-state performance rather than cold-start overhead. Following warmup, the suite executes multiple measurement iterations on standardized datasets representing different data characteristics (text, binary, compressible logs, random data), capturing detailed timing for each iteration and computing statistical summaries including mean, median, standard deviation, minimum, and maximum throughput values. The `BenchmarkResult` class encapsulates benchmark outcomes, storing operation type, dataset characteristics (size, entropy, type), service implementation tested (CPU or GPU), timing measurements for overall and stage-level performance, compression ratio achieved, and success/failure status. The module implements comparative analysis through the `BenchmarkComparison` class, which pairs CPU and GPU results for identical workloads and computes speedup factors, performance consistency metrics (coefficient of variation), and statistical significance tests to determine whether observed differences exceed measurement noise. Benchmark reports are generated in multiple formats including human-readable console output with formatted tables, CSV export for spreadsheet analysis and graphing, and JSON serialization for programmatic processing and integration with continuous integration systems. This systematic benchmarking approach provides the empirical foundation for performance claims in the thesis, such as the 2.61× GPU speedup for frequency analysis and 1.08× overall speedup, and enables ongoing performance regression detection as the codebase evolves. The module's design emphasizes reproducibility through controlled test conditions, deterministic data generation, and comprehensive measurement methodology documentation.

## 7. Config Module

The Config Module (`com.datacomp.config`) centralizes application configuration management, providing persistent storage of user preferences and runtime settings that control compression behavior, resource allocation, and logging verbosity. The `AppConfig` class implements the Singleton pattern, ensuring a single global configuration instance accessible throughout the application while preventing inconsistent state from multiple configuration objects. Configuration persistence is achieved through Java's `Properties` mechanism, serializing settings to a `.datacomp.properties` file in the user's home directory and loading preferences on application startup, preserving user choices across sessions. Managed settings include chunk size (controlling memory usage and parallelism granularity), default service type (CPU-only, GPU-hybrid, or automatic selection based on hardware availability), parallel thread count for CPU Fork/Join operations, GPU device selection in multi-GPU systems, decode table size (balancing memory usage vs decoding speed), compression level presets (fast/balanced/maximum), and logging configuration (level, output destination, format). The module provides type-safe getters and setters with validation logic that enforces constraints such as chunk size power-of-two requirements, thread count within available CPU core limits, and decode table size within memory bounds. Configuration changes trigger listener notifications through the observer pattern, allowing UI components to update displayed settings and service implementations to reconfigure runtime parameters without requiring application restart. The module also defines sensible default values that ensure correct operation when configuration files are absent or corrupted, such as 16MB default chunk size, automatic service selection with GPU preference, thread count equal to available CPU cores, and INFO-level logging. This centralized configuration architecture eliminates hardcoded constants scattered throughout the codebase, facilitates user customization of performance-critical parameters, supports A/B testing of different configuration strategies during benchmarking, and provides a clear extension point for adding future configuration options such as custom compression algorithms, network compression support, or cloud storage integration settings.
